# Init Folder

This folder contains initialization scripts and configurations for setting up databases used in the POC Data Engineering project.

## Overview

The init folder provides setup scripts for different database systems that are part of the data pipeline. These scripts handle initial database configuration, schema creation, and data loading from processed Parquet files.

## Docker Integration

The initialization scripts are integrated into the Docker Compose setup defined in `docker-compose.dbt.yml`:

### PostgreSQL Service Integration
- **Volume Mount**: `./init/postgres:/docker-entrypoint-initdb.d`
- **Automatic Execution**: Scripts run automatically when PostgreSQL container starts for the first time
- **Data Source**: Parquet files are copied from `./parquet_data/data/` to `/opt/parquet/` in the container
- **Dependencies**: DuckDB CLI is installed in the PostgreSQL container for data conversion

### DuckDB Service (Commented Out)
- **Volume Mount**: `./init/duckdb:/init` (when enabled)
- **Initialization**: Uses `init.sql` to create initial database schema
- **Data Volume**: Mounts `./duckdb-data:/data` for persistent storage

## Subdirectories

### 1. DuckDB (`duckdb/`)
Contains initialization scripts for DuckDB, an embedded analytical database.

#### Files:
- **`init.sql`**: Basic database initialization script that creates a test table
  - Creates a simple `test` table with `id` (integer) and `name` (nvarchar) columns
  - Used for testing DuckDB connectivity and basic operations

#### Usage:
DuckDB can be initialized by running the SQL script directly or through DuckDB's command-line interface.

### 2. PostgreSQL (`postgres/`)
Contains setup scripts for PostgreSQL database initialization and data loading.

#### Files:
- **`0-setup.sh`**: Comprehensive setup script for loading processed data into PostgreSQL
  - Converts Parquet files to JSON format using DuckDB
  - Creates a target table in PostgreSQL with JSONB column
  - Loads the converted JSON data into the database

#### Script Workflow:
1. **Data Conversion**: Uses DuckDB to convert Parquet files (`/opt/parquet/*.parquet`) to JSON format
2. **Schema Preparation**: Creates the target table `all_data` with a JSONB column in PostgreSQL
3. **Data Loading**: Imports the JSON data into PostgreSQL using the COPY command
4. **Completion**: Confirms successful loading of all data

#### Configuration:
- **Source**: Parquet files from `/opt/parquet/` directory (mounted from host `./parquet_data/data/`)
- **Intermediate**: JSON file at `/tmp/data.json`
- **Target Table**: `all_data` (configurable via `TARGET_TABLE` variable)
- **Database Connection**: Uses environment variables (`POSTGRES_USER`, `POSTGRES_DB`, `POSTGRES_PASSWORD`)

#### Usage:
This script is designed to run as a PostgreSQL initialization script (typically placed in `/docker-entrypoint-initdb.d/` in Docker containers). It automatically processes and loads data when the PostgreSQL container starts.

## Integration with Data Pipeline

- **Input**: Consumes Parquet files generated by the consumer module
- **Processing**: Converts columnar Parquet data to JSON for relational storage
- **Output**: Structured data in PostgreSQL ready for querying and analysis

## Dependencies

- **DuckDB**: Required for data format conversion (installed in PostgreSQL container)
- **PostgreSQL**: Target database for data storage
- **Environment Variables**: Database connection parameters must be set
- **Docker Compose**: Orchestrates the database services and volume mounting

## Notes

- The PostgreSQL setup script assumes a Docker environment with proper volume mounting
- Parquet files should be available in the expected location before running the setup
- The JSONB storage allows flexible querying of the nested event data structures
- Scripts are executed only on first container startup (not on restarts)